{"cells":[{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":193,"status":"ok","timestamp":1697169291268,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"umHD0Ph8PAWG"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.compose import make_column_selector, ColumnTransformer\n","from sklearn.impute import SimpleImputer\n","from sklearn.linear_model import LinearRegression\n","from sklearn.pipeline import FeatureUnion, Pipeline\n","from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n","\n","from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess"]},{"cell_type":"markdown","metadata":{"id":"ejd1xOxd4xKR"},"source":["# Functions for loading CSV files as DataFrames:\n","Each of these functions below reads the data from a CSV file, processes it (e.g., parsing dates, setting index columns), and returns a Pandas DataFrame containing the data for further analysis. These functions provide a convenient way to load and prepare various types of data for analysis in a structured format.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":235,"status":"ok","timestamp":1697169291749,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"6nwdhqh64kkp"},"outputs":[],"source":["def load_train_df(filepath, onpromotion=False) -\u003e pd.DataFrame:\n","    if onpromotion:\n","        _usecols = [\"id\", \"store_nbr\", \"family\", \"date\", \"sales\", \"onpromotion\"]\n","    else:\n","        _usecols = [\"id\", \"store_nbr\", \"family\", \"date\", \"sales\"]\n","\n","    df = (\n","        pd.read_csv(filepath, usecols=_usecols, parse_dates=[\"date\"], index_col=[\"store_nbr\", \"family\", \"date\"],\n","                    dtype={\"store_nbr\": np.uint8,\"sales\": np.float32,})\n","        .sort_index(axis=0)\n","        .sort_index(axis=1))\n","    return df\n","\n","\n","def load_test_df(filepath, onpromotion=False) -\u003e pd.DataFrame:\n","    if onpromotion:\n","        _usecols = [\"id\", \"store_nbr\", \"family\", \"date\", \"onpromotion\"]\n","    else:\n","        _usecols = [\"id\", \"store_nbr\", \"family\", \"date\"]\n","\n","    df = (\n","        pd.read_csv(filepath, usecols=_usecols, parse_dates=[\"date\"], index_col=[\"store_nbr\", \"family\", \"date\"],\n","                       dtype={\"store_nbr\": np.uint8,\"sales\": np.float32,})\n","        .sort_index(axis=0)\n","        .sort_index(axis=1))\n","    return df\n","\n","def load_transactions_df(filepath) -\u003e pd.DataFrame:\n","    df = (\n","        pd.read_csv(filepath, parse_dates=[\"date\"],\n","                    dtype={\"store_nbr\": np.uint8,\"transactions\": np.float32,})\n","        .pipe(lambda df: df.assign(date=df.loc[:, \"date\"].dt.to_period('D'))).set_index([\"store_nbr\", \"date\"])\n","        .sort_index(axis=0)\n","        .sort_index(axis=1))\n","    return df\n","\n","\n","def load_oil_df(filepath) -\u003e pd.DataFrame:\n","    df = (\n","        pd.read_csv(filepath, parse_dates=[\"date\"],index_col=[\"date\"],\n","                    dtype={\"dcoilwtico\": np.float32,})\n","        .fillna(method=\"bfill\")   # fills value for 2013-01-01 only!\n","        .sort_index(axis=0))\n","    return df\n","\n","\n","def load_stores_df(filepath) -\u003e pd.DataFrame:\n","    df = (\n","        pd.read_csv(filepath, index_col=[\"store_nbr\"],\n","                    dtype={\"store_nbr\": np.uint8,\"cluster\": np.uint8,})\n","        .rename(columns={\"type\": \"store_type\", \"cluster\": \"store_cluster\"})\n","        .sort_index(axis=0)\n","        .sort_index(axis=1))\n","    return df\n","\n","\n","def load_holidays_events_df(filepath) -\u003e pd.DataFrame:\n","    df = (\n","        pd.read_csv(filepath, parse_dates=[\"date\"],)\n","        .sort_index(axis=0)\n","        .sort_index(axis=1)\n","    )\n","    return df"]},{"cell_type":"markdown","metadata":{"id":"Auu2IZNx594w"},"source":["# **Data Preprocessing Pipeline**"]},{"cell_type":"markdown","metadata":{"id":"SrNYggmp6FeB"},"source":["***Pipeline for cleaning the training data:***\n","The quality of the data prior to mid-2015 is poor for many of the data series so I decided to drop it; many of the remaining series have significant numbers of leading zero values for sales. These leading zero sales values are better coded as missing values. Replace all the leading zero values for sales and then drop all the missing values."]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1697169291749,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"yx1g-gNp6kG7"},"outputs":[],"source":["def _all_false_mask(df):\n","    mask_df = pd.DataFrame(False, columns=df.columns, index=df.index)\n","    return mask_df\n","\n","\n","def _create_leading_zero_sales_mask(df):\n","    mask_df = _all_false_mask(df)\n","    sales_all_zero = df.loc[:, 'sales'].eq(0).all()\n","    if not sales_all_zero:\n","        mask_df['sales'] = df.loc[:, 'sales'].eq(0).cummin()\n","    return mask_df\n","\n","\n","def _drop_leading_zero_sales(df):\n","    masked_df = _mask_leading_zero_sales(df)\n","    without_leading_zeros_df = (masked_df.dropna(how=\"any\", subset=[\"sales\"], axis=0))\n","    return without_leading_zeros_df\n","\n","\n","def _mask_leading_zero_sales(df):\n","    cond = df.groupby(level=[\"store_nbr\", \"family\"], group_keys=False).apply(lambda df: _create_leading_zero_sales_mask(df))\n","    masked_df = df.mask(cond, np.nan)\n","    return masked_df\n","\n","\n","def _use_only_recent_data(df, start):\n","    recent_df = df.loc[pd.IndexSlice[:, :, start:]]\n","    return recent_df\n","\n","def _make_data_cleaning_pipeline(start, verbose=True) -\u003e Pipeline:\n","    pipeline = Pipeline(\n","        steps=[\n","            (f\"Use only data after {start}\", FunctionTransformer(func=_use_only_recent_data, kw_args={\"start\": start})),\n","            (\"Drop leading zero sales values\", FunctionTransformer(func=_drop_leading_zero_sales,)),],\n","        verbose=verbose,).set_output(transform=\"pandas\")\n","\n","    return pipeline"]},{"cell_type":"markdown","metadata":{"id":"ukxO30tQ9CFs"},"source":["# _make_data_cleaning_pipeline(start, verbose=True):\n","\n"," This function creates a data cleaning pipeline using scikit-learn's Pipeline class.\n","It includes two steps in the pipeline: The first step uses the _use_only_recent_data function to filter the DataFrame to include only data\n","after the start date. The second step uses the _drop_leading_zero_sales function to drop rows with leading zero sales values.\n","The verbose parameter controls whether the pipeline prints verbose information. The pipeline is configured to output the result as a Pandas DataFrame.\n","\n","\n","---\n","\n","\n","# Pipeline for joining the data sets\n","\n","\n","After cleaning the training data, join all the test.csv, stores.csv, oil.csv, and transactions.csv. Leave the holidays_events.csv alone for now as it will be added later during the feature engineering step."]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1697169291749,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"S0Pwk8gk-Ef3"},"outputs":[],"source":["def _load_and_join_oil_df(df, filepath):\n","    oil_df = load_oil_df(filepath)\n","    joined_df = (df.join(oil_df, how=\"left\", on=[\"date\"]).sort_index(axis=0).sort_index(axis=1))\n","\n","    return joined_df\n","\n","\n","def _load_and_join_stores_df(df, filepath):\n","    stores_df = load_stores_df(filepath)\n","    joined_df = (df.join(stores_df, how=\"left\", on=[\"store_nbr\"]).sort_index(axis=0).sort_index(axis=1))\n","    return joined_df\n","\n","\n","def _load_and_concat_test_df(df, filepath, onpromotion):\n","    test_df = load_test_df(filepath, onpromotion)\n","    concat_df = (pd.concat([df, test_df], axis=0, sort=True,).sort_index(axis=0))\n","    return concat_df\n","\n","\n","def _load_and_join_transactions_df(df, filepath):\n","    transactions_df = load_transactions_df(filepath)\n","    joined_df = (df.join(transactions_df, how=\"left\", on=[\"store_nbr\", \"date\"]).sort_index(axis=0).sort_index(axis=1))\n","    return joined_df\n","\n","\n","def _make_oil_joining_pipeline(filepath, verbose):\n","    pipeline = Pipeline(\n","        steps=[\n","            (\"Load and join the data\", FunctionTransformer(func=_load_and_join_oil_df, kw_args={\"filepath\": filepath})),\n","            (\"Impute missing values\", ColumnTransformer(transformers=[(\"Forward fill missing dcoilwtico values\",\n","                            FunctionTransformer(lambda df: df.fillna(method=\"ffill\")),[\"dcoilwtico\"]),],\n","                                                        n_jobs=1, remainder=\"passthrough\",\n","                                                        verbose=verbose, verbose_feature_names_out=False)),],\n","        verbose=verbose)\n","    return pipeline\n","\n","\n","def _make_transactions_joining_pipeline(filepath, verbose):\n","    pipeline = Pipeline(\n","        steps=[\n","            (\"Load and join the data\", FunctionTransformer(func=_load_and_join_transactions_df, kw_args={\"filepath\": filepath}),),\n","            (\"Impute missing values\", ColumnTransformer(transformers=[(\"Fill missing transactions with zeros\",\n","                            SimpleImputer(strategy=\"constant\", fill_value=0.0), [\"transactions\"])],\n","                                                        n_jobs=1, remainder=\"passthrough\",\n","                                                        verbose=verbose, verbose_feature_names_out=False,))],\n","        verbose=verbose)\n","    return pipeline\n","\n","\n","def _make_data_joining_pipeline(onpromotion, join_oil, join_transactions, verbose):\n","\n","    steps = []\n","    if join_transactions:\n","        _pipeline = _make_transactions_joining_pipeline(\"transactions.csv\", verbose)\n","        steps.append((\"Load and join the transactions data\", _pipeline))\n","\n","    if join_oil:\n","        _pipeline = _make_oil_joining_pipeline(\"oil.csv\", verbose)\n","        steps.append((\"Load and join the oil price data\", _pipeline))\n","\n","    # step to concatenate the test data\n","    steps.append((\"Load and concatenate the test data\",\n","            FunctionTransformer(\n","                func=_load_and_concat_test_df,\n","                kw_args={\n","                    \"filepath\": \"test.csv\",\n","                    \"onpromotion\": onpromotion,})))\n","\n","    # step to join the stores data\n","    steps.append((\"Load and join the stores data\",\n","            FunctionTransformer(\n","                func=_load_and_join_stores_df,\n","                kw_args={\"filepath\": \"stores.csv\"})))\n","\n","    pipeline = Pipeline(steps, verbose=verbose).set_output(transform=\"pandas\")\n","\n","    return pipeline"]},{"cell_type":"markdown","metadata":{"id":"Zoc2mYm1Anmi"},"source":["# Full preprocessing pipeline\n","\n","The full preprocessing pipeline combines the cleaning steps, the joining steps, with a final step which clips outliers.\n"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":312,"status":"ok","timestamp":1697169292058,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"dBlvnCFRAw-F"},"outputs":[],"source":["def _clip_outliers(df, q):\n","    expanding_quantiles_df = (\n","        df.groupby(level=[\"store_nbr\", \"family\"], group_keys=False)\n","          .apply(lambda _: _compute_expanding_quantiles(_, q))\n","    )\n","    mask = _all_false_mask(df)\n","    is_outlier = df.gt(expanding_quantiles_df).to_dict()\n","    cond = mask.assign(**is_outlier)\n","    clipped_df = df.mask(cond, expanding_quantiles_df)\n","    return clipped_df\n","\n","def _compute_expanding_quantiles(df, q):\n","    expanding_quantiles = (\n","        df.expanding(min_periods=1)\n","          .quantile(q)\n","          .to_dict()\n","    )\n","    expanding_quantile_df = (\n","        df.assign(**expanding_quantiles)\n","          .astype(np.float32)\n","    )\n","    return expanding_quantile_df\n","\n","# Transformers and Pipelines\n","\n","def _make_clip_outliers_transformer(onpromotion, dcoilwtico, transactions, q, n_jobs, verbose):\n","    transformers = []\n","\n","    if onpromotion:\n","        transformers.append(\n","            (\n","                \"Clip onpromotion outliers\",\n","                FunctionTransformer(\n","                    func=_clip_outliers,\n","                    kw_args={'q': q}\n","                ),\n","                [\"onpromotion\"]\n","            ))\n","    if dcoilwtico:\n","        transformers.append(\n","            (\"Clip dcoilwtico outliers\",\n","                FunctionTransformer(\n","                    func=_clip_outliers,\n","                    kw_args={'q': q}\n","                ),\n","                [\"dcoilwtico\"]\n","            ))\n","    if transactions:\n","        transformers.append(\n","            (\"Clip transactions outliers\",\n","                FunctionTransformer(\n","                    func=_clip_outliers,\n","                    kw_args={'q': q}\n","                ),\n","                [\"transactions\"]))\n","\n","    transformer = ColumnTransformer(\n","        transformers,\n","        n_jobs=n_jobs,\n","        remainder=\"passthrough\",\n","        verbose=verbose,\n","        verbose_feature_names_out=False\n","    ).set_output(transform=\"pandas\")\n","\n","    return transformer\n","\n","def make_data_preprocessing_pipeline(onpromotion=False, join_oil=True, join_transactions=True,\n","                                     start=\"20150701\", q=0.99, n_jobs=-1, verbose=True) -\u003e Pipeline:\n","    pipeline = Pipeline(\n","        steps=[\n","            (\"Clean the training data\",\n","                _make_data_cleaning_pipeline(start, verbose)\n","            ),\n","            (\"Join the additional data sets\",\n","                _make_data_joining_pipeline(\n","                    onpromotion,\n","                    join_oil,\n","                    join_transactions,\n","                    verbose\n","                )),\n","            (\"Clip outliers\",\n","                _make_clip_outliers_transformer(\n","                    onpromotion,\n","                    join_oil,\n","                    join_transactions,\n","                    q,\n","                    n_jobs,\n","                    verbose\n","                )),\n","            (\n","                \"Sort the columns\",\n","                FunctionTransformer(\n","                    func=lambda df: df.sort_index(axis=1)\n","                )\n","            )\n","        ],\n","        verbose=verbose\n","    ).set_output(transform=\"pandas\")\n","\n","    return pipeline\n","\n","def _load_and_join_oil_df(df, filepath):\n","    oil_df = load_oil_df(filepath)\n","    joined_df = (df.join(oil_df, how=\"left\", on=[\"date\"]).sort_index(axis=0).sort_index(axis=1))\n","    return joined_df\n","\n","def _load_and_join_stores_df(df, filepath):\n","    stores_df = load_stores_df(filepath)\n","    joined_df = (df.join(stores_df, how=\"left\", on=[\"store_nbr\"]).sort_index(axis=0).sort_index(axis=1))\n","    return joined_df\n","\n","def _load_and_concat_test_df(df, filepath, onpromotion):\n","    test_df = load_test_df(filepath, onpromotion)\n","    concat_df = (pd.concat([df, test_df], axis=0, sort=True,).sort_index(axis=0))\n","    return concat_df\n","\n","def _load_and_join_transactions_df(df, filepath):\n","    transactions_df = load_transactions_df(filepath)\n","    joined_df = (df.join(transactions_df, how=\"left\", on=[\"store_nbr\", \"date\"]).sort_index(axis=0).sort_index(axis=1))\n","    return joined_df\n","\n","# Pipelines for Data Joining\n","\n","def _make_oil_joining_pipeline(filepath, verbose):\n","    pipeline = Pipeline(\n","        steps=[\n","            (\"Load and join the data\", FunctionTransformer(func=_load_and_join_oil_df, kw_args={\"filepath\": filepath})),\n","            (\"Impute missing values\", ColumnTransformer(transformers=[(\"Forward fill missing dcoilwtico values\",\n","                FunctionTransformer(lambda df: df.fillna(method=\"ffill\")),[\"dcoilwtico\"]),],\n","                n_jobs=1, remainder=\"passthrough\", verbose=verbose, verbose_feature_names_out=False)),\n","        ],\n","        verbose=verbose\n","    )\n","    return pipeline\n","\n","def _make_transactions_joining_pipeline(filepath, verbose):\n","    pipeline = Pipeline(\n","        steps=[\n","            (\"Load and join the data\", FunctionTransformer(func=_load_and_join_transactions_df, kw_args={\"filepath\": filepath}),),\n","            (\"Impute missing values\", ColumnTransformer(transformers=[(\"Fill missing transactions with zeros\",\n","                SimpleImputer(strategy=\"constant\", fill_value=0.0), [\"transactions\"])],\n","                n_jobs=1, remainder=\"passthrough\", verbose=verbose, verbose_feature_names_out=False,)\n","            )\n","        ],\n","        verbose=verbose\n","    )\n","    return pipeline\n","\n","def _make_data_joining_pipeline(onpromotion, join_oil, join_transactions, verbose):\n","    steps = []\n","\n","    if join_transactions:\n","        _pipeline = _make_transactions_joining_pipeline(\"transactions.csv\", verbose)\n","        steps.append((\"Load and join the transactions data\", _pipeline))\n","\n","    if join_oil:\n","        _pipeline = _make_oil_joining_pipeline(\"oil.csv\", verbose)\n","        steps.append((\"Load and join the oil price data\", _pipeline))\n","\n","    steps.append((\"Load and concatenate the test data\",\n","        FunctionTransformer(\n","            func=_load_and_concat_test_df,\n","            kw_args={\"filepath\": \"test.csv\", \"onpromotion\": onpromotion}\n","        )\n","    ))\n","\n","    steps.append((\"Load and join the stores data\",\n","        FunctionTransformer(\n","            func=_load_and_join_stores_df,\n","            kw_args={\"filepath\": \"stores.csv\"}\n","        )\n","    ))\n","\n","    pipeline = Pipeline(steps, verbose=verbose).set_output(transform=\"pandas\")\n","    return pipeline\n"]},{"cell_type":"markdown","metadata":{"id":"UkUTLmY9B2aa"},"source":["\n","\n","_clip_outliers(df, q): This function clips outliers in the input DataFrame df based on a quantile q. It first computes expanding quantiles for each group defined by the \"store_nbr\" and \"family\" columns and then masks the outliers in the original DataFrame with the corresponding quantile values.\n","\n","_compute_expanding_quantiles(df, q): This function computes expanding quantiles for the input DataFrame df based on the specified quantile q. It calculates the quantiles in an expanding window fashion and returns a DataFrame with quantile values.\n","\n","_make_clip_outliers_transformer(onpromotion, dcoilwtico, transactions, q, n_jobs, verbose): This function creates a transformer for clipping outliers in specific columns of a DataFrame. The columns to be processed are determined by the input parameters (onpromotion, dcoilwtico, transactions). For each specified column, it adds a transformer to clip outliers using the _clip_outliers function.\n","\n","make_data_preprocessing_pipeline(onpromotion, join_oil, join_transactions, start, q, n_jobs, verbose): This is the main function that creates the overall data preprocessing pipeline. It consists of several steps:\n","\n","\"Clean the training data\": Uses _make_data_cleaning_pipeline to clean and filter the data.\n","\"Join the additional data sets\": Uses _make_data_joining_pipeline to join additional data sets such as oil prices, transactions, and store information.\n","\"Clip outliers\": Uses _make_clip_outliers_transformer to clip outliers in specified columns.\n","\"Sort the columns\": Sorts the columns of the final DataFrame.\n","The pipeline is configured to output the final preprocessed DataFrame.\n","\n","\n","\n","---\n","\n","# Example usage\n"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64949,"status":"ok","timestamp":1697169357004,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"o1XGl5HDB9Gp","outputId":"b804d173-9789-4e38-fb57-644a1970ebe2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_function_transformer.py:343: UserWarning: With transform=\"pandas\", `func` should return a DataFrame to follow the set_output API.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[Pipeline]  (step 1 of 2) Processing Use only data after 20150701, total=   0.0s\n","[Pipeline]  (step 2 of 2) Processing Drop leading zero sales values, total=   2.6s\n","[Pipeline]  (step 1 of 4) Processing Clean the training data, total=   2.7s\n","[Pipeline]  (step 1 of 2) Processing Load and join the data, total=   5.1s\n","[ColumnTransformer]  (1 of 2) Processing Fill missing transactions with zeros, total=   0.0s\n","[ColumnTransformer] ..... (2 of 2) Processing remainder, total=   0.0s\n","[Pipeline]  (step 2 of 2) Processing Impute missing values, total=   0.0s\n","[Pipeline]  (step 1 of 4) Processing Load and join the transactions data, total=   5.1s\n","[Pipeline]  (step 1 of 2) Processing Load and join the data, total=   0.3s\n","[ColumnTransformer]  (1 of 2) Processing Forward fill missing dcoilwtico values, total=   0.0s\n","[ColumnTransformer] ..... (2 of 2) Processing remainder, total=   0.0s\n","[Pipeline]  (step 2 of 2) Processing Impute missing values, total=   0.0s\n","[Pipeline]  (step 2 of 4) Processing Load and join the oil price data, total=   0.4s\n","[Pipeline]  (step 3 of 4) Processing Load and concatenate the test data, total=   0.5s\n","[Pipeline]  (step 4 of 4) Processing Load and join the stores data, total=   0.8s\n","[Pipeline]  (step 2 of 4) Processing Join the additional data sets, total=   6.8s\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[Pipeline] ..... (step 3 of 4) Processing Clip outliers, total=  48.9s\n","[Pipeline] .. (step 4 of 4) Processing Sort the columns, total=   0.1s\n"]}],"source":["_train_df = load_train_df(\n","    \"train.csv\",\n","    onpromotion=True,\n",")\n","_pipeline = make_data_preprocessing_pipeline()\n","_preprocessed_df = _pipeline.fit_transform(_train_df)"]},{"cell_type":"markdown","metadata":{"id":"e8HfpcK5Dp7V"},"source":["Note that the future values for dcoilwtico, onpromotion, and transactions would not be available at the time that we need to generate our sales forecasts in a real forecasting setting. For the purposes of the competition we are given the values for dcoilwtico and onpromotion for the test window. We are not given the transactions values and will need to forecast them."]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":663,"status":"ok","timestamp":1697169357832,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"GuB5hq6wD6Zt","outputId":"a31b8a44-4039-4c29-e53e-f98a4c5bd1f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003cclass 'pandas.core.frame.DataFrame'\u003e\n","MultiIndex: 1353218 entries, (1, 'AUTOMOTIVE', Timestamp('2015-07-01 00:00:00')) to (54, 'SEAFOOD', Timestamp('2017-08-31 00:00:00'))\n","Data columns (total 9 columns):\n"," #   Column         Non-Null Count    Dtype  \n","---  ------         --------------    -----  \n"," 0   city           1353218 non-null  object \n"," 1   dcoilwtico     1324706 non-null  float32\n"," 2   id             1353218 non-null  int64  \n"," 3   onpromotion    1324706 non-null  float64\n"," 4   sales          1324706 non-null  float32\n"," 5   state          1353218 non-null  object \n"," 6   store_cluster  1353218 non-null  uint8  \n"," 7   store_type     1353218 non-null  object \n"," 8   transactions   1324706 non-null  float32\n","dtypes: float32(3), float64(1), int64(1), object(3), uint8(1)\n","memory usage: 73.6+ MB\n"]}],"source":["# values for dcoilwtico, onpromotion, and transactions are missing\n","_preprocessed_df.info()"]},{"cell_type":"markdown","metadata":{"id":"y4z1vMSMEAJ_"},"source":["#Feature Engineering Pipeline\n","\n","---\n","\n","***Date Features***"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1697169357832,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"8UZg7ltLEKp1"},"outputs":[],"source":["def _create_date_features(df):\n","    _df = df.reset_index(\"date\")\n","    date_features = {\n","        \"year\": _df.loc[:, \"date\"].dt.year,\n","        \"month\": _df.loc[:, \"date\"].dt.month.astype(np.uint8),\n","        \"day\": _df.loc[:, \"date\"].dt.day,\n","        \"day_of_week\": _df.loc[:, \"date\"].dt.day_of_week.astype(np.uint8),\n","        \"is_month_start\": _df.loc[:, \"date\"].dt.is_month_start.astype(np.uint8),\n","        \"is_month_middle\": _df.loc[:, \"date\"].dt.day.eq(15).astype(np.uint8),\n","        \"is_month_end\": _df.loc[:, \"date\"].dt.is_month_end.astype(np.uint8),\n","        \"is_weekend\": _df.loc[:, \"date\"].dt.day_of_week.floordiv(5).astype(np.uint8),\n","    }\n","    with_date_features_df = (\n","        _df.assign(**date_features)\n","           .set_index(\"date\", append=True)\n","           .sort_index(axis=0)\n","           .sort_index(axis=1)\n","    )\n","    return with_date_features_df\n","\n","\n","def make_date_features_transformer(verbose=True):\n","    transformer = ColumnTransformer(\n","        transformers=[(\"Engineer date features\",\n","                       FunctionTransformer(func=_create_date_features,), [\"id\"])],\n","        remainder=\"drop\",\n","        verbose=verbose,\n","        verbose_feature_names_out=False\n","        ).set_output(transform=\"pandas\")\n","    return transformer"]},{"cell_type":"markdown","metadata":{"id":"7J_UoS8PEngs"},"source":["_create_date_features(df): This function takes a DataFrame as input and creates several date-related features from the \"date\" column. It does the following:\n","\n","Resets the index of the DataFrame to include \"date\" as a regular column.\n","Extracts various date-related features such as year, month, day, day of the week, and others from the \"date\" column using the .dt accessor provided by Pandas.\n","Assigns these new features to the DataFrame as additional columns.\n","Sets the \"date\" column back as the index and sorts the DataFrame.\n","The resulting DataFrame contains the original data along with the newly engineered date-related features.\n","\n","make_date_features_transformer(verbose=True): This function creates a transformer that can be used within a data preprocessing pipeline to add date-related features to a DataFrame. It uses scikit-learn's ColumnTransformer and FunctionTransformer to apply the _create_date_features function to the DataFrame. The \"id\" column is specified as the input column, although it is not used for the feature engineering. The remainder parameter is set to \"drop,\" which means that all columns other than the specified input column will be dropped.\n","\n","The transformer is configured to output the transformed DataFrame.\n","\n","Overall, these functions are useful for adding date-related features to a DataFrame, which can be valuable for time series analysis and modeling tasks. The make_date_features_transformer function provides a convenient way to incorporate this feature engineering step into a data preprocessing pipeline.\n","\n","\n","---\n","\n","#Holidays and Events Features\n","\n","I make heavy use of Pandas string processing methods to extract a unique description for each holiday at the national, regional, and local level. I will use these descriptions to create indicator/dummy variables and add them as features in our model."]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1697169357833,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"Jclurzy8Eprk"},"outputs":[],"source":["_HOLIDAYS_EVENTS_DF = load_holidays_events_df(\"holidays_events.csv\")\n","\n","\n","HOLIDAY_TYPES = [\"Additional\", \"Bridge\", \"Event\", \"Holiday\", \"Transfer\", \"Work Day\"]\n","\n","\n","LOCAL_HOLIDAY_DESCRIPTIONS = (\n","    _HOLIDAYS_EVENTS_DF.query(\"(locale == 'Local') \u0026 (type in @HOLIDAY_TYPES) \u0026 (~transferred)\")\n","                       .loc[:, \"description\"]\n","                       .str\n","                       .removeprefix(\"Traslado \")\n","                       .str\n","                       .strip(\"-+0123456789\")\n","                       .unique()\n","                       .tolist()\n",")\n","\n","NATIONAL_HOLIDAY_DESCRIPTIONS = (\n","    _HOLIDAYS_EVENTS_DF.query(\"(locale == 'National') \u0026 (type in @HOLIDAY_TYPES) \u0026 (~transferred)\")\n","                       .loc[:, \"description\"]\n","                       .str\n","                       .removeprefix(\"Traslado \")\n","                       .str\n","                       .removeprefix(\"Puente \")\n","                       .str\n","                       .removeprefix(\"Inauguracion \")\n","                       .str\n","                       .replace(\":.*\", '', regex=True)\n","                       .str\n","                       .replace(\"puente\", \"Puente\")\n","                       .str\n","                       .replace(\"primer\", \"Primer\")\n","                       .str\n","                       .strip(\"-+0123456789\")\n","                       .unique()\n","                       .tolist()\n",")\n","\n","\n","REGIONAL_HOLIDAY_DESCRIPTIONS = (\n","    _HOLIDAYS_EVENTS_DF.query(\"(locale == 'Regional') \u0026 (type in @HOLIDAY_TYPES) \u0026 (~transferred)\")\n","                       .loc[:, \"description\"]\n","                       .unique()\n","                       .tolist()\n",")"]},{"cell_type":"markdown","metadata":{"id":"VF4GHTIvFIeL"},"source":["I define some functions which will use the unique descriptions and create an indicator for each individual holiday. I need separate functions for the different locales as the joining logic is different."]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1697169357833,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"RWPPkFtJFK4y"},"outputs":[],"source":["def _create_some_local_holiday_indicator(df, description):\n","    is_local_holiday = (\n","        _HOLIDAYS_EVENTS_DF.query(f\"(locale == 'Local') \u0026 (type in {HOLIDAY_TYPES}) \u0026 (~transferred)\")\n","                           .loc[:, \"description\"]\n","                           .str\n","                           .contains(description, case=False)\n","    )\n","    indicator_name = f\"is_{description.lower().replace(' ', '_')}\"\n","    is_local_holiday_df = (\n","        _HOLIDAYS_EVENTS_DF.query(f\"(locale == 'Local') \u0026 (type in {HOLIDAY_TYPES}) \u0026 (~transferred)\")\n","                           .loc[is_local_holiday, [\"date\", \"locale_name\"]]\n","                           .rename(columns={\"locale_name\": \"city\"})\n","                           .assign(**{indicator_name: 1})\n","                           .drop_duplicates()\n","                           .set_index([\"date\", \"city\"])\n","    )\n","    with_local_holiday_indicator_df = (\n","        df.join(\n","            is_local_holiday_df,\n","            how=\"left\",\n","            on=[\"date\", \"city\"]\n","        ).fillna(\n","            value={indicator_name: 0}\n","        ).astype(\n","            {indicator_name: np.uint8}\n","        ).sort_index(\n","            axis=1\n","        )\n","    )\n","    return with_local_holiday_indicator_df\n","\n","\n","def _create_some_national_holiday_indicator(df, description):\n","    is_national_holiday = (\n","        _HOLIDAYS_EVENTS_DF.query(f\"(locale == 'National') \u0026 (type in {HOLIDAY_TYPES}) \u0026 (~transferred)\")\n","                           .loc[:, \"description\"]\n","                           .str\n","                           .contains(description, case=False)\n","    )\n","    indicator_name = f\"is_{description.lower().replace(' ', '_')}\"\n","    is_national_holiday_df = (\n","        _HOLIDAYS_EVENTS_DF.query(f\"(locale == 'National') \u0026 (type in {HOLIDAY_TYPES}) \u0026 (~transferred)\")\n","                           .loc[is_national_holiday, [\"date\"]]\n","                           .assign(**{indicator_name: 1})\n","                           .drop_duplicates()\n","                           .set_index(\"date\")\n","    )\n","    with_national_holiday_indicator_df = (\n","        df.join(\n","            is_national_holiday_df,\n","            how=\"left\",\n","            on=[\"date\"]\n","        ).fillna(\n","            value={indicator_name: 0}\n","        ).astype(\n","            {indicator_name: np.uint8}\n","        ).sort_index(\n","            axis=1\n","        )\n","    )\n","    return with_national_holiday_indicator_df\n","\n","\n","def _create_some_regional_holiday_indicator(df, description):\n","    is_regional_holiday = (\n","        _HOLIDAYS_EVENTS_DF.query(f\"(locale == 'Regional') \u0026 (type in {HOLIDAY_TYPES}) \u0026 (~transferred)\")\n","                           .loc[:, \"description\"]\n","                           .str\n","                           .contains(description, case=False)\n","    )\n","    indicator_name = f\"is_{description.lower().replace(' ', '_')}\"\n","    is_regional_holiday_df = (\n","        _HOLIDAYS_EVENTS_DF.query(f\"(locale == 'Regional') \u0026 (type in {HOLIDAY_TYPES}) \u0026 (~transferred)\")\n","                           .loc[is_regional_holiday, [\"date\", \"locale_name\"]]\n","                           .rename(columns={\"locale_name\": \"state\"})\n","                           .assign(**{indicator_name: 1})\n","                           .drop_duplicates()\n","                           .set_index([\"date\", \"state\"])\n","    )\n","    with_regional_holiday_indicator_df = (\n","        df.join(\n","            is_regional_holiday_df,\n","            how=\"left\",\n","            on=[\"date\", \"state\"]\n","        ).fillna(\n","            value={indicator_name: 0}\n","        ).astype(\n","            {indicator_name: np.uint8}\n","        ).sort_index(\n","            axis=1\n","        )\n","    )\n","    return with_regional_holiday_indicator_df"]},{"cell_type":"markdown","metadata":{"id":"t9ZoUFh1FkTW"},"source":["\n","These functions are used to create holiday indicators based on specific descriptions and types of holidays from a holiday events DataFrame (_HOLIDAYS_EVENTS_DF). These holiday indicators are added to a given DataFrame and help capture whether a particular holiday is present or not for each date or date and location (city or state). Let's break down what each function does:\n","\n","_create_some_local_holiday_indicator(df, description): This function creates a local holiday indicator based on a specific holiday description. It does the following:\n","\n","Filters the holiday events DataFrame _HOLIDAYS_EVENTS_DF to select holidays of type \"Local\" that match the specified description and are not transferred.\n","Constructs the name of the indicator column based on the description.\n","Creates a DataFrame (is_local_holiday_df) with columns \"date,\" \"city,\" and the indicator column.\n","Joins this indicator DataFrame with the input DataFrame df on \"date\" and \"city.\"\n","Fills missing values with 0 (indicating no holiday) and converts the indicator column to uint8 data type.\n","Sorts the resulting DataFrame.\n","_create_some_national_holiday_indicator(df, description): This function is similar to the previous one but creates a national holiday indicator instead. It filters holidays of type \"National\" in the holiday events DataFrame and follows a similar process to create and join the indicator.\n","\n","_create_some_regional_holiday_indicator(df, description): This function creates a regional holiday indicator based on the specified description. It filters holidays of type \"Regional\" in the holiday events DataFrame and follows a similar process to create and join the indicator. The indicator is associated with a specific state.\n","\n","Each of these functions allows you to create holiday indicators for different types of holidays (local, national, regional) and specific holiday descriptions. These indicators are useful in time series analysis and modeling to account for the impact of holidays on the data.\n","\n","It's important to note that these functions work with a pre-defined holiday events DataFrame (_HOLIDAYS_EVENTS_DF) and are designed to be used within a larger data preprocessing pipeline to enhance the feature set of the data with holiday-related information.\n","\n","\n","---\n","Now I define functions that build column transformers for each locale's holidays and events. Using column transformers allows my to parallelize the feature creation process."]},{"cell_type":"code","execution_count":59,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1697169357833,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"ZafBjuQkF3Zk"},"outputs":[],"source":["def _make_local_holidays_transformer(n_jobs=-1, verbose=True):\n","    transformers = []\n","    for description in LOCAL_HOLIDAY_DESCRIPTIONS:\n","        transformers.append(\n","            (\n","                f\"Create {description} indicator\",\n","                FunctionTransformer(\n","                    func=_create_some_local_holiday_indicator,\n","                    kw_args={\"description\": description}\n","                ),\n","                [\"city\"]\n","            )\n","        )\n","\n","    transformer = ColumnTransformer(\n","        transformers,\n","        n_jobs=n_jobs,\n","        remainder=\"drop\",\n","        verbose=verbose,\n","        verbose_feature_names_out=False\n","    ).set_output(transform=\"pandas\")\n","\n","    return transformer\n","\n","\n","def _make_national_holidays_transformer(n_jobs=-1, verbose=True):\n","    transformers = []\n","    for description in NATIONAL_HOLIDAY_DESCRIPTIONS:\n","        transformers.append(\n","            (\n","                f\"Create {description} indicator\",\n","                FunctionTransformer(\n","                    func=_create_some_national_holiday_indicator,\n","                    kw_args={\"description\": description}\n","                ),\n","                [\"id\"]\n","            )\n","        )\n","\n","    transformer = ColumnTransformer(\n","        transformers,\n","        n_jobs=n_jobs,\n","        remainder=\"drop\",\n","        verbose=verbose,\n","        verbose_feature_names_out=False\n","    ).set_output(transform=\"pandas\")\n","\n","    return transformer\n","\n","\n","def _make_regional_holidays_transformer(n_jobs=-1, verbose=True):\n","    transformers = []\n","    for description in REGIONAL_HOLIDAY_DESCRIPTIONS:\n","        transformers.append(\n","            (\n","                f\"Create {description} indicator\",\n","                FunctionTransformer(\n","                    func=_create_some_regional_holiday_indicator,\n","                    kw_args={\"description\": description}\n","                ),\n","                [\"state\"]\n","            )\n","        )\n","\n","    transformer = ColumnTransformer(\n","        transformers,\n","        n_jobs=n_jobs,\n","        remainder=\"drop\",\n","        verbose=verbose,\n","        verbose_feature_names_out=False\n","    ).set_output(transform=\"pandas\")\n","\n","    return transformer"]},{"cell_type":"markdown","metadata":{"id":"WgXc9up4F74Q"},"source":["Next, I use a FeatureUnion to join together the three ColumnTransformer objects. This adds a further level of parallelism because now each of these three transformers can be computed in parallel."]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1697169357833,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"zg89f6CEGB5x"},"outputs":[],"source":["def _create_local_holiday_indicator(df):\n","    indicator = (\n","        df.select_dtypes(include=np.uint8)\n","          .max(axis=1)\n","    )\n","    with_indicator_df = (\n","        df.assign(is_local_holiday=indicator)\n","    )\n","    return with_indicator_df\n","\n","\n","def _create_national_holiday_indicator(df):\n","    indicator = (\n","        df.select_dtypes(include=np.uint8)\n","          .max(axis=1)\n","    )\n","    with_indicator_df = (\n","        df.assign(is_national_holiday=indicator)\n","    )\n","    return with_indicator_df\n","\n","\n","def _create_regional_holiday_indicator(df):\n","    indicator = (\n","        df.select_dtypes(include=np.uint8)\n","          .max(axis=1)\n","    )\n","    with_indicator_df = (\n","        df.assign(is_regional_holiday=indicator)\n","    )\n","    return with_indicator_df\n","\n","\n","def _remove_duplicated_columns(df):\n","    return df.loc[:, ~df.columns.duplicated()]\n","\n","\n","def make_holidays_events_feature_union(n_jobs=-1, verbose=True):\n","    feature_union = FeatureUnion(\n","        transformer_list=[\n","            (\n","                \"Engineer local holiday features\",\n","                Pipeline(\n","                    steps=[\n","                        (\n","                            \"Engineer individual local holiday indicators\",\n","                            _make_local_holidays_transformer(\n","                                n_jobs=-1,\n","                                verbose=verbose,\n","                            )\n","                        ),\n","                        (\n","                            \"Remove any duplicated columns\",\n","                            FunctionTransformer(\n","                                func=_remove_duplicated_columns\n","                            )\n","                        ),\n","                        (\n","                            \"Engineer local holiday indicator\",\n","                            FunctionTransformer(\n","                                func=_create_local_holiday_indicator\n","                            )\n","                        )\n","                    ],\n","                    verbose=verbose\n","                )\n","            ),\n","            (\n","                \"Engineer national holiday and event features\",\n","                Pipeline(\n","                    steps=[\n","                        (\n","                            \"Engineer indivdual national holiday and event indicators\",\n","                            _make_national_holidays_transformer(\n","                                n_jobs=-1,\n","                                verbose=verbose\n","                            )\n","                        ),\n","                        (\n","                            \"Remove any duplicated columns\",\n","                            FunctionTransformer(\n","                                func=_remove_duplicated_columns\n","                            )\n","                        ),\n","                        (\n","                            \"Engineer national holiday and event indicator\",\n","                            FunctionTransformer(\n","                                func=_create_national_holiday_indicator\n","                            )\n","                        )\n","                    ],\n","                    verbose=verbose\n","                )\n","            ),\n","            (\n","                \"Engineer regional holiday features\",\n","                Pipeline(\n","                    steps=[\n","                        (\n","                            \"Engineer individual regional holiday indicators\",\n","                            _make_regional_holidays_transformer(\n","                                n_jobs=-1,\n","                                verbose=verbose\n","                            )\n","                        ),\n","                        (\n","                            \"Remove any duplicated columns\",\n","                            FunctionTransformer(\n","                                func=_remove_duplicated_columns\n","                            )\n","                        ),\n","                        (\n","                            \"Engineer regional holiday indicator\",\n","                            FunctionTransformer(\n","                                func=_create_regional_holiday_indicator\n","                            )\n","                        )\n","                    ],\n","                    verbose=verbose\n","                )\n","            )\n","        ],\n","        n_jobs=n_jobs,\n","        verbose=verbose\n","    ).set_output(transform=\"pandas\")\n","\n","    return feature_union"]},{"cell_type":"markdown","metadata":{"id":"XVkPsiEMGJ5V"},"source":["The code you provided defines several functions and a feature union that is used to create holiday-related indicators and features from a given DataFrame. Here's a breakdown of the key components and what this code is doing:\n","\n","_create_local_holiday_indicator(df): This function creates a binary indicator for local holidays. It checks for columns with uint8 data type in the input DataFrame df and takes the maximum value across rows. The result is a binary indicator column \"is_local_holiday\" that is added to the DataFrame.\n","\n","_create_national_holiday_indicator(df): Similar to the previous function, this one creates a binary indicator for national holidays based on uint8 columns in the input DataFrame. It adds an \"is_national_holiday\" column to the DataFrame.\n","\n","_create_regional_holiday_indicator(df): This function creates a binary indicator for regional holidays using uint8 columns in the input DataFrame. It adds an \"is_regional_holiday\" column to the DataFrame.\n","\n","_remove_duplicated_columns(df): This utility function removes duplicated columns from the input DataFrame df. It ensures that there are no duplicate columns after adding holiday indicators.\n","\n","make_holidays_events_feature_union(n_jobs=-1, verbose=True): This function creates a feature union using scikit-learn's FeatureUnion class. The feature union combines multiple pipelines for engineering holiday-related features. It consists of three main parts:\n","\n","\"Engineer local holiday features\": This pipeline first generates individual local holiday indicators using _make_local_holidays_transformer, removes duplicated columns, and then creates an overall \"is_local_holiday\" indicator using _create_local_holiday_indicator.\n","\"Engineer national holiday and event features\": This pipeline follows a similar pattern to create national holiday and event indicators.\n","\"Engineer regional holiday features\": This pipeline is dedicated to creating regional holiday indicators.\n","The feature union combines these three pipelines, and the resulting feature union can be used to engineer holiday-related features from a DataFrame. The n_jobs and verbose parameters control parallel processing and verbosity during feature engineering.\n","\n","\n","---\n","\n","#Example Usage"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15314,"status":"ok","timestamp":1697169373143,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"uxXHxVhJHB4o","outputId":"927edc5a-73df-4332-cef6-ade18df29e44"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_function_transformer.py:343: UserWarning: With transform=\"pandas\", `func` should return a DataFrame to follow the set_output API.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n"]}],"source":["_feature_union = make_holidays_events_feature_union(\n","    n_jobs=-1,\n","    verbose=True\n",")\n","_df = _feature_union.fit_transform(_preprocessed_df)"]},{"cell_type":"markdown","metadata":{"id":"1Ri6xZwQHfMD"},"source":["#Lagged Feature Engineering Pipeline\n","Notice that in order to properly create the lagged features from the transactions data that we will need to train our model and then forecast sales, we will need a model to forecast the missing transactions over the test window. Typically we would also need models for other features like dcoilwtico and onpromotion (and any other features we use during training that are not known at the time we make our forecast)."]},{"cell_type":"code","execution_count":62,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1697169373143,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"XksdUoy8HwnQ"},"outputs":[],"source":["def _totals_by_store_nbr(df, onpromotion, transactions):\n","    features = [\"sales\"]\n","    if onpromotion:\n","        features.append(\"onpromotion\")\n","    if transactions:\n","        features.append(transactions)\n","    totals_df = (\n","        df.loc[:, features]\n","          .groupby([\"store_nbr\", \"date\"])\n","          .sum()\n","          .rename(columns={feature: f\"total_{feature}\" for feature in features})\n","    )\n","    return totals_df\n","\n","\n","def _sales_per_transaction(df):\n","    totals_df = _totals_by_store_nbr(df, onpromotion=False, transactions=True)\n","    sales_per_transaction = (\n","        totals_df.loc[:, \"sales\"]\n","                 .div(totals_df.loc[:, \"transactions\"])\n","    )\n","    return sales_per_transaction"]},{"cell_type":"code","execution_count":63,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1697169373144,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"ewsUjX8oLB2p"},"outputs":[],"source":["def _create_lagged_features(s, lags, drop):\n","    feature = s.name\n","    lagged_features = {}\n","    for lag in lags:\n","        lagged_features[f\"{feature}_lag{lag:02d}\"] = (\n","            s.groupby([\"store_nbr\", \"family\"])\n","             .shift(periods=lag)\n","             .fillna(method=\"bfill\")\n","             .astype(np.float32)\n","        )\n","    if drop:\n","        with_lagged_features_df = (\n","            s.to_frame()\n","             .assign(**lagged_features)\n","             .drop(feature, axis=1)\n","             .sort_index(axis=1)\n","        )\n","    else:\n","        with_lagged_features_df = (\n","            s.to_frame()\n","             .assign(**lagged_features)\n","             .sort_index(axis=1)\n","        )\n","    return with_lagged_features_df\n","\n","\n","def _create_rolling_mean(s, lag, window):\n","    rolling_mean = {\n","        f\"{s.name}_{window:02d}_day_rolling_mean\": (\n","            s.groupby([\"store_nbr\", \"family\"])\n","             .rolling(min_periods=1, window=window)\n","             .mean()\n","             .astype(np.float32)\n","             .droplevel([0, 1])\n","        )\n","    }\n","    with_rolling_mean_df = (\n","        s.to_frame()\n","         .assign(**rolling_mean)\n","         .sort_index(axis=1)\n","    )\n","    return with_rolling_mean_df\n","\n","\n","def _fillna_with_forecast(df, forecast_df):\n","    with_forecast_df = (\n","        df.fillna(forecast_df)\n","    )\n","    return with_forecast_df\n","\n","\n","def _make_fillna_with_forecast_transformer(\n","    dcoilwtico_forecast_df,\n","    onpromotion_forecast_df,\n","    transactions_forecast_df,\n","    n_jobs,\n","    verbose):\n","\n","    transformer = ColumnTransformer(\n","        transformers=[\n","            (\n","                \"Insert dcoilwtico forecast\",\n","                FunctionTransformer(\n","                    func=_fillna_with_forecast,\n","                    kw_args={\n","                        \"forecast_df\": dcoilwtico_forecast_df\n","                    }\n","                ),\n","                [\"dcoilwtico\"]\n","            ),\n","            (\n","                \"Insert onpromotion forecast\",\n","                FunctionTransformer(\n","                    func=_fillna_with_forecast,\n","                    kw_args={\n","                        \"forecast_df\": onpromotion_forecast_df\n","                    }\n","                ),\n","                [\"onpromotion\"]\n","            ),\n","            (\n","                \"Insert transactions forecast\",\n","                FunctionTransformer(\n","                    func=_fillna_with_forecast,\n","                    kw_args={\n","                        \"forecast_df\": transactions_forecast_df\n","                    }\n","                ),\n","                [\"transactions\"]\n","            )\n","        ],\n","        n_jobs=n_jobs,\n","        remainder=\"drop\",\n","        verbose=verbose,\n","        verbose_feature_names_out=False\n","    ).set_output(transform=\"pandas\")\n","\n","    return transformer\n","\n","\n","def _make_lagged_feature_transformer(\n","    max_lags,\n","    n_jobs,\n","    verbose):\n","    transformer = ColumnTransformer(\n","        transformers=[\n","            (\n","                \"Engineer lagged dcoilwtico features\",\n","                FunctionTransformer(\n","                    func=_create_lagged_features,\n","                    kw_args={\n","                        \"lags\": range(1, max_lags + 1),\n","                        \"drop\": True,\n","                    }\n","                ),\n","                \"dcoilwtico\"\n","            ),\n","            (\n","                \"Engineer lagged onpromotion features\",\n","                FunctionTransformer(\n","                    func=_create_lagged_features,\n","                    kw_args={\n","                        \"lags\": range(1, max_lags + 1),\n","                        \"drop\": True,\n","                    }\n","                ),\n","                \"onpromotion\"\n","            ),\n","            (\n","                \"Engineer lagged transactions features\",\n","                FunctionTransformer(\n","                    func=_create_lagged_features,\n","                    kw_args={\n","                        \"lags\": range(1, max_lags + 1),\n","                        \"drop\": True,\n","                    }\n","                ),\n","                \"transactions\"\n","            ),\n","        ],\n","        n_jobs=n_jobs,\n","        remainder=\"drop\",\n","        verbose=verbose,\n","        verbose_feature_names_out=False\n","    )\n","    return transformer\n","\n","\n","def _make_rolling_means_transformer(\n","    lag,\n","    window,\n","    n_jobs,\n","    verbose) -\u003e ColumnTransformer:\n","    transformer = ColumnTransformer(\n","        transformers=[\n","            (\n","                f\"Engineer dcoilwtico_lag{lag:02d} {window:02d}-day rolling mean\",\n","                FunctionTransformer(\n","                    func=_create_rolling_mean,\n","                    kw_args={\n","                        \"lag\": lag,\n","                        \"window\": window\n","                    }\n","                ),\n","                f\"dcoilwtico_lag{lag:02d}\"\n","            ),\n","            (\n","                f\"Engineer onpromotion_lag{lag:02d} {window:02d}-day rolling mean\",\n","                FunctionTransformer(\n","                    func=_create_rolling_mean,\n","                    kw_args={\n","                        \"lag\": lag,\n","                        \"window\": window\n","                    }\n","                ),\n","                f\"onpromotion_lag{lag:02d}\"\n","            ),\n","            (\n","                f\"Engineer transactions_lag{lag:02d} {window:02d}-day rolling mean\",\n","                FunctionTransformer(\n","                    func=_create_rolling_mean,\n","                    kw_args={\n","                        \"lag\": lag,\n","                        \"window\": window\n","                    }\n","                ),\n","                f\"transactions_lag{lag:02d}\"\n","            ),\n","        ],\n","        n_jobs=n_jobs,\n","        remainder=\"passthrough\",\n","        verbose=verbose,\n","        verbose_feature_names_out=False\n","    ).set_output(transform=\"pandas\")\n","\n","    return transformer\n","\n","\n","def make_lagged_feature_engineering_pipeline(\n","    max_lags,\n","    dcoilwtico_forecast_df,\n","    onpromotion_forecast_df,\n","    transactions_forecast_df,\n","    n_jobs=-1,\n","    verbose=True) -\u003e Pipeline:\n","    pipeline = Pipeline(\n","        steps=[\n","            (\n","                \"Insert test window forecasts\",\n","                _make_fillna_with_forecast_transformer(\n","                    dcoilwtico_forecast_df,\n","                    onpromotion_forecast_df,\n","                    transactions_forecast_df,\n","                    n_jobs,\n","                    verbose,\n","                ),\n","            ),\n","            (\n","                \"Create lagged features\",\n","                _make_lagged_feature_transformer(\n","                    max_lags,\n","                    n_jobs,\n","                    verbose\n","                )\n","            ),\n","            (\n","                \"Create weekly rolling means\",\n","                _make_rolling_means_transformer(\n","                    1,\n","                    7,\n","                    n_jobs,\n","                    verbose\n","                )\n","            ),\n","            (\n","                \"Create monthly rolling means\",\n","                _make_rolling_means_transformer(\n","                    1,\n","                    28,\n","                    n_jobs,\n","                    verbose\n","                )\n","            ),\n","        ],\n","        verbose=verbose,\n","    ).set_output(transform=\"pandas\")\n","\n","    return pipeline"]},{"cell_type":"markdown","metadata":{"id":"h61t5449HvdR"},"source":["The provided code defines a set of functions and a pipeline for feature engineering and lagging for time series data. Here's an explanation of what each function does and how they contribute to the pipeline:\n","\n","_create_lagged_features(s, lags, drop): This function creates lagged features for a given time series column s. It generates lagged versions of the column with different time periods specified in the lags list. If drop is True, it drops the original column and only keeps the lagged features.\n","\n","_create_rolling_mean(s, lag, window): This function calculates rolling mean features for a given time series column s. It calculates the rolling mean over a specified window size and assigns the result to a new column with a descriptive name.\n","\n","_fillna_with_forecast(df, forecast_df): This function fills missing values in a DataFrame df with corresponding values from a forecast DataFrame forecast_df.\n","\n","_make_fillna_with_forecast_transformer(dcoilwtico_forecast_df, onpromotion_forecast_df, transactions_forecast_df, n_jobs, verbose): This function creates a transformer to fill missing values in specific columns of a DataFrame using forecast data. It handles columns like \"dcoilwtico,\" \"onpromotion,\" and \"transactions.\"\n","\n","_make_lagged_feature_transformer(max_lags, n_jobs, verbose): This function creates a transformer to generate lagged features for columns such as \"dcoilwtico,\" \"onpromotion,\" and \"transactions\" up to a maximum number of lags specified by max_lags.\n","\n","_make_rolling_means_transformer(lag, window, n_jobs, verbose): This function creates a transformer to calculate rolling mean features for columns with lagged data. It computes rolling means with a specific window size.\n","\n","make_lagged_feature_engineering_pipeline(max_lags, dcoilwtico_forecast_df, onpromotion_forecast_df, transactions_forecast_df, n_jobs=-1, verbose=True): This function creates a feature engineering pipeline that consists of the following steps:\n","\n","Filling missing values with forecast data using _make_fillna_with_forecast_transformer.\n","Generating lagged features using _make_lagged_feature_transformer.\n","Creating weekly rolling mean features using _make_rolling_means_transformer.\n","Creating monthly rolling mean features using _make_rolling_means_transformer.\n","The pipeline allows for the inclusion of forecast DataFrames for \"dcoilwtico,\" \"onpromotion,\" and \"transactions.\" It also supports parallel processing (n_jobs) and verbosity control (verbose).\n","\n","\n","---\n","# Example Usgae\n","\n","We are given data on dcoilwtico and onpromotion for the test window. However, for a real deployment of our forecasting pipeline we would NOT know the future values for either the price of oil nor number of products on promotion nor transactions. If we want to use these series to create features for training our forecasting pipeline, then we need to also be able to generate forecasts of these variables prior to generating predictions from our model.\n"]},{"cell_type":"code","execution_count":64,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1697169373144,"user":{"displayName":"Lucas Giumarra","userId":"10486241603689822620"},"user_tz":420},"id":"Pti6tvpgIQlM"},"outputs":[],"source":["def forecast_dcoilwtico():\n","    test_df = load_test_df(\"test.csv\")\n","    oil_df = load_oil_df(\"oil.csv\")\n","    forecast_df = (\n","        test_df.join(oil_df, how=\"left\", on=[\"date\"])\n","               .groupby([\"store_nbr\", \"family\"])\n","               .fillna(method=\"ffill\")\n","               .loc[:, [\"dcoilwtico\"]]\n","    )\n","    return forecast_df\n","\n","\n","def forecast_onpromotion():\n","    test_df = load_test_df(\"test.csv\", onpromotion=True)\n","    forecast_df = (\n","        test_df.loc[:, [\"onpromotion\"]]\n","    )\n","    return forecast_df\n","\n","\n","def forecast_transactions():\n","    test_df = load_test_df(\"test.csv\")\n","    train_df = load_train_df(\"train.csv\")\n","    transactions_df = load_transactions_df(\"transactions.csv\")\n","\n","    joined_df = (\n","        train_df.join(\n","            transactions_df,\n","            how=\"left\",\n","            on=[\"store_nbr\", \"date\"]\n","        )\n","    )\n","    concat_df = (\n","        pd.concat(\n","            [joined_df, test_df],\n","            axis=0,\n","            sort=True,\n","        ).sort_index(axis=0)\n","    )\n","    forecast_df = (\n","        concat_df.groupby([\"store_nbr\", \"family\"])\n","                 .fillna(method=\"ffill\")\n","                 .loc[pd.IndexSlice[:, :, \"2017-08-16\":], [\"transactions\"]]\n","    )\n","    return forecast_df"]},{"cell_type":"markdown","metadata":{"id":"bcR50nd6Il11"},"source":["\n","The provided code defines three functions for forecasting values for different features using the test dataset. Each function reads the test data and performs a forecast for a specific feature:\n","\n","forecast_dcoilwtico(): This function forecasts the \"dcoilwtico\" feature. It loads the test data and the oil price data, joins them based on the \"date\" column, and fills missing values using forward fill (method=\"ffill\"). Then, it selects the \"dcoilwtico\" column and returns a DataFrame with the forecasted values for \"dcoilwtico.\"\n","\n","forecast_onpromotion(): This function forecasts the \"onpromotion\" feature. It loads the test data with \"onpromotion\" information, and then it selects only the \"onpromotion\" column. The function returns a DataFrame with the forecasted values for \"onpromotion.\"\n","\n","forecast_transactions(): This function forecasts the \"transactions\" feature. It loads the train data, transactions data, and the test data. It joins the train data and transactions data based on the \"store_nbr\" and \"date\" columns. It concatenates the train and test data, sorts it, and fills missing values using forward fill (method=\"ffill\"). Finally, it selects the \"transactions\" column for dates starting from \"2017-08-16\" and returns a DataFrame with the forecasted values for \"transactions.\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Z7961L-KIlRh"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_function_transformer.py:343: UserWarning: With transform=\"pandas\", `func` should return a DataFrame to follow the set_output API.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[Pipeline]  (step 1 of 4) Processing Insert test window forecasts, total=  12.5s\n","[Pipeline]  (step 2 of 4) Processing Create lagged features, total=  22.0s\n","[Pipeline]  (step 3 of 4) Processing Create weekly rolling means, total=   6.0s\n","[Pipeline]  (step 4 of 4) Processing Create monthly rolling means, total=   6.2s\n"]}],"source":["_pipeline = make_lagged_feature_engineering_pipeline(\n","    max_lags=28,\n","    dcoilwtico_forecast_df=forecast_dcoilwtico(),\n","    onpromotion_forecast_df=forecast_onpromotion(),\n","    transactions_forecast_df=forecast_transactions(),\n","    n_jobs=-1,\n","    verbose=True\n",")\n","_df = _pipeline.fit_transform(_preprocessed_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"grQHTYCdMasf"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003cclass 'pandas.core.frame.DataFrame'\u003e\n","MultiIndex: 1353218 entries, (1, 'AUTOMOTIVE', Timestamp('2015-07-01 00:00:00')) to (54, 'SEAFOOD', Timestamp('2017-08-31 00:00:00'))\n","Data columns (total 90 columns):\n"," #   Column                                  Non-Null Count    Dtype  \n","---  ------                                  --------------    -----  \n"," 0   dcoilwtico_lag01                        1353218 non-null  float32\n"," 1   dcoilwtico_lag01_28_day_rolling_mean    1353218 non-null  float32\n"," 2   onpromotion_lag01                       1353218 non-null  float32\n"," 3   onpromotion_lag01_28_day_rolling_mean   1353218 non-null  float32\n"," 4   transactions_lag01                      1353203 non-null  float32\n"," 5   transactions_lag01_28_day_rolling_mean  1353218 non-null  float32\n"," 6   dcoilwtico_lag01_07_day_rolling_mean    1353218 non-null  float32\n"," 7   onpromotion_lag01_07_day_rolling_mean   1353218 non-null  float32\n"," 8   transactions_lag01_07_day_rolling_mean  1353209 non-null  float32\n"," 9   dcoilwtico_lag02                        1353218 non-null  float32\n"," 10  dcoilwtico_lag03                        1353218 non-null  float32\n"," 11  dcoilwtico_lag04                        1353218 non-null  float32\n"," 12  dcoilwtico_lag05                        1353218 non-null  float32\n"," 13  dcoilwtico_lag06                        1353218 non-null  float32\n"," 14  dcoilwtico_lag07                        1353218 non-null  float32\n"," 15  dcoilwtico_lag08                        1353218 non-null  float32\n"," 16  dcoilwtico_lag09                        1353218 non-null  float32\n"," 17  dcoilwtico_lag10                        1353218 non-null  float32\n"," 18  dcoilwtico_lag11                        1353218 non-null  float32\n"," 19  dcoilwtico_lag12                        1353218 non-null  float32\n"," 20  dcoilwtico_lag13                        1353218 non-null  float32\n"," 21  dcoilwtico_lag14                        1353218 non-null  float32\n"," 22  dcoilwtico_lag15                        1353218 non-null  float32\n"," 23  dcoilwtico_lag16                        1353218 non-null  float32\n"," 24  dcoilwtico_lag17                        1353218 non-null  float32\n"," 25  dcoilwtico_lag18                        1353218 non-null  float32\n"," 26  dcoilwtico_lag19                        1353218 non-null  float32\n"," 27  dcoilwtico_lag20                        1353218 non-null  float32\n"," 28  dcoilwtico_lag21                        1353218 non-null  float32\n"," 29  dcoilwtico_lag22                        1353218 non-null  float32\n"," 30  dcoilwtico_lag23                        1353218 non-null  float32\n"," 31  dcoilwtico_lag24                        1353218 non-null  float32\n"," 32  dcoilwtico_lag25                        1353218 non-null  float32\n"," 33  dcoilwtico_lag26                        1353218 non-null  float32\n"," 34  dcoilwtico_lag27                        1353218 non-null  float32\n"," 35  dcoilwtico_lag28                        1353218 non-null  float32\n"," 36  onpromotion_lag02                       1353218 non-null  float32\n"," 37  onpromotion_lag03                       1353218 non-null  float32\n"," 38  onpromotion_lag04                       1353218 non-null  float32\n"," 39  onpromotion_lag05                       1353218 non-null  float32\n"," 40  onpromotion_lag06                       1353218 non-null  float32\n"," 41  onpromotion_lag07                       1353218 non-null  float32\n"," 42  onpromotion_lag08                       1353218 non-null  float32\n"," 43  onpromotion_lag09                       1353218 non-null  float32\n"," 44  onpromotion_lag10                       1353218 non-null  float32\n"," 45  onpromotion_lag11                       1353218 non-null  float32\n"," 46  onpromotion_lag12                       1353218 non-null  float32\n"," 47  onpromotion_lag13                       1353218 non-null  float32\n"," 48  onpromotion_lag14                       1353218 non-null  float32\n"," 49  onpromotion_lag15                       1353218 non-null  float32\n"," 50  onpromotion_lag16                       1353218 non-null  float32\n"," 51  onpromotion_lag17                       1353218 non-null  float32\n"," 52  onpromotion_lag18                       1353218 non-null  float32\n"," 53  onpromotion_lag19                       1353218 non-null  float32\n"," 54  onpromotion_lag20                       1353218 non-null  float32\n"," 55  onpromotion_lag21                       1353218 non-null  float32\n"," 56  onpromotion_lag22                       1353218 non-null  float32\n"," 57  onpromotion_lag23                       1353218 non-null  float32\n"," 58  onpromotion_lag24                       1353218 non-null  float32\n"," 59  onpromotion_lag25                       1353218 non-null  float32\n"," 60  onpromotion_lag26                       1353218 non-null  float32\n"," 61  onpromotion_lag27                       1353218 non-null  float32\n"," 62  onpromotion_lag28                       1353218 non-null  float32\n"," 63  transactions_lag02                      1353204 non-null  float32\n"," 64  transactions_lag03                      1353205 non-null  float32\n"," 65  transactions_lag04                      1353206 non-null  float32\n"," 66  transactions_lag05                      1353207 non-null  float32\n"," 67  transactions_lag06                      1353208 non-null  float32\n"," 68  transactions_lag07                      1353209 non-null  float32\n"," 69  transactions_lag08                      1353210 non-null  float32\n"," 70  transactions_lag09                      1353211 non-null  float32\n"," 71  transactions_lag10                      1353212 non-null  float32\n"," 72  transactions_lag11                      1353213 non-null  float32\n"," 73  transactions_lag12                      1353214 non-null  float32\n"," 74  transactions_lag13                      1353215 non-null  float32\n"," 75  transactions_lag14                      1353216 non-null  float32\n"," 76  transactions_lag15                      1353217 non-null  float32\n"," 77  transactions_lag16                      1353218 non-null  float32\n"," 78  transactions_lag17                      1353218 non-null  float32\n"," 79  transactions_lag18                      1353218 non-null  float32\n"," 80  transactions_lag19                      1353218 non-null  float32\n"," 81  transactions_lag20                      1353218 non-null  float32\n"," 82  transactions_lag21                      1353218 non-null  float32\n"," 83  transactions_lag22                      1353218 non-null  float32\n"," 84  transactions_lag23                      1353218 non-null  float32\n"," 85  transactions_lag24                      1353218 non-null  float32\n"," 86  transactions_lag25                      1353218 non-null  float32\n"," 87  transactions_lag26                      1353218 non-null  float32\n"," 88  transactions_lag27                      1353218 non-null  float32\n"," 89  transactions_lag28                      1353218 non-null  float32\n","dtypes: float32(90)\n","memory usage: 469.8+ MB\n"]}],"source":["_df.info()"]},{"cell_type":"markdown","metadata":{"id":"mPD8-xCeMimE"},"source":["#Pipeline to encode categorical features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Rd3ZAcgLM424"},"outputs":[],"source":["def make_feature_encoding_transformer(n_jobs=-1, verbose=True):\n","    transformer = ColumnTransformer(\n","        transformers=[\n","            (\n","                \"One-hot encode city feature\",\n","                OneHotEncoder(\n","                    sparse_output=False,\n","                    dtype=np.uint8\n","                ),\n","                [\"city\"]\n","            ),\n","            (\n","                \"One-hot encode state feature\",\n","                OneHotEncoder(\n","                    sparse_output=False,\n","                    dtype=np.uint8\n","                ),\n","                [\"state\"]\n","            ),\n","            (\n","                \"One-hot encode store_cluster feature\",\n","                OneHotEncoder(\n","                    sparse_output=False,\n","                    dtype=np.uint8\n","                ),\n","                [\"store_cluster\"]\n","            ),\n","            (\n","                \"One-hot encode store_type feature\",\n","                OneHotEncoder(\n","                    sparse_output=False,\n","                    dtype=np.uint8\n","                ),\n","                [\"store_type\"]\n","            )\n","        ],\n","        n_jobs=n_jobs,\n","        remainder=\"drop\",\n","        verbose=verbose,\n","        verbose_feature_names_out=False,\n","    ).set_output(transform=\"pandas\")\n","\n","    return transformer"]},{"cell_type":"markdown","metadata":{"id":"VzVPvTliMyX-"},"source":["#Combined Feature Engineering Pipeline\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"JZr7qEOzNhfY"},"outputs":[],"source":["def make_feature_engineering_feature_union(\n","    max_lags,\n","    dcoilwtico_forecast_df,\n","    onpromotion_forecast_df,\n","    transactions_forecast_df,\n","    n_jobs=-1,\n","    verbose=True) -\u003e FeatureUnion:\n","\n","    feature_union = FeatureUnion(\n","        transformer_list=[\n","            (\n","                \"Passthrough sales target\",\n","                ColumnTransformer(\n","                    transformers=[\n","                        (\n","                            \"Identity transform\",\n","                            FunctionTransformer(\n","                                func=lambda df: df\n","                            ),\n","                            [\"sales\"]\n","                        )\n","                    ],\n","                    n_jobs=1,\n","                    remainder=\"drop\",\n","                    verbose=verbose,\n","                    verbose_feature_names_out=False,\n","                ),\n","            ),\n","            (\n","                \"Engineer date features\",\n","                make_date_features_transformer(\n","                    verbose\n","                )\n","            ),\n","            (\n","                \"Engineer holidays and events features\",\n","                make_holidays_events_feature_union(\n","                    n_jobs,\n","                    verbose\n","                )\n","            ),\n","            (\n","                \"Engineer lagged features\",\n","                make_lagged_feature_engineering_pipeline(\n","                    max_lags,\n","                    dcoilwtico_forecast_df,\n","                    onpromotion_forecast_df,\n","                    transactions_forecast_df,\n","                    n_jobs,\n","                    verbose\n","                )\n","            ),\n","            (\n","                \"Encode categorical features\",\n","                make_feature_encoding_transformer(\n","                    n_jobs=n_jobs,\n","                    verbose=verbose\n","                )\n","            ),\n","        ],\n","        n_jobs=n_jobs,\n","        verbose=verbose\n","    ).set_output(transform=\"pandas\")\n","\n","    return feature_union\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Gdm7wKLcNr8E"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_function_transformer.py:343: UserWarning: With transform=\"pandas\", `func` should return a DataFrame to follow the set_output API.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["\u003cclass 'pandas.core.frame.DataFrame'\u003e\n","MultiIndex: 1353218 entries, (1, 'AUTOMOTIVE', Timestamp('2015-07-01 00:00:00')) to (54, 'SEAFOOD', Timestamp('2017-08-31 00:00:00'))\n","Columns: 211 entries, sales to store_type_E\n","dtypes: float32(91), int64(4), object(2), uint8(114)\n","memory usage: 684.0+ MB\n"]}],"source":["_feature_union_kwargs = {\n","    \"max_lags\": 28,\n","    \"dcoilwtico_forecast_df\": forecast_dcoilwtico(),\n","    \"onpromotion_forecast_df\": forecast_onpromotion(),\n","    \"transactions_forecast_df\": forecast_transactions(),\n","    \"n_jobs\": -1,\n","    \"verbose\": True\n","}\n","_feature_union = make_feature_engineering_feature_union(\n","    **_feature_union_kwargs\n",")\n","_df = _feature_union.fit_transform(_preprocessed_df)\n","_df.info()"]},{"cell_type":"markdown","metadata":{"id":"u_Gmn-3yN54p"},"source":["#Pipeline for converting to Nixlats format"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6zBn-qgBN5Pz"},"outputs":[],"source":["def _create_unique_id(df):\n","    unique_ids = (\n","        df.loc[:, [\"store_nbr\", \"family\"]]\n","          .apply(lambda feature: feature.cat.codes, axis=0)\n","          .apply(lambda cs: '_'.join(str(c) for c in cs), axis=1)\n","          .astype(\"category\")\n","    )\n","    return unique_ids\n","\n","\n","def make_nixtlats_preparation_pipeline(verbose=True) -\u003e Pipeline:\n","    pipeline = Pipeline(\n","        steps=[\n","            (\n","                \"Reset multi-index\",\n","                FunctionTransformer(\n","                    func=lambda df: df.reset_index()\n","                )\n","            ),\n","            (\n","                \"Convert store_nbr and family to category dtype\",\n","                FunctionTransformer(\n","                    func=lambda df: df.astype({\"store_nbr\": \"category\", \"family\": \"category\"})\n","                )\n","            ),\n","            (\n","                \"Create unique_id column using store_nbr and family\",\n","                FunctionTransformer(\n","                    func=lambda df: df.assign(unique_id=_create_unique_id(df))\n","                )\n","            ),\n","            (\n","                \"Encode remaining categorical features\",\n","                ColumnTransformer(\n","                    transformers=[\n","                        (\n","                            \"One-hot encode store_nbr and family\",\n","                            OneHotEncoder(\n","                                sparse_output=False,\n","                                dtype=np.uint8\n","                            ),\n","                            [\"store_nbr\", \"family\"]\n","                        )\n","                    ],\n","                    remainder=\"passthrough\",\n","                    verbose=verbose,\n","                    verbose_feature_names_out=False,\n","                )\n","            ),\n","            (\n","                \"Rename columns for use with nixlats libraries\",\n","                FunctionTransformer(\n","                    func=lambda df: df.rename(columns={\"date\": \"ds\", \"sales\": 'y'})\n","                )\n","            ),\n","            (\n","                \"Sort the columns\",\n","                FunctionTransformer(\n","                    func=lambda df: df.sort_index(axis=1)\n","                )\n","            ),\n","            (\n","                \"Split into train and test data sets\",\n","                FunctionTransformer(\n","                    func=lambda df: (df.query(\"ds \u003c 20170816\"), df.query(\"ds \u003e= 20170816\"))\n","                )\n","            )\n","        ],\n","        verbose=verbose\n","    ).set_output(transform=\"pandas\")\n","\n","    return pipeline"]},{"cell_type":"markdown","metadata":{"id":"9_DqhjDZNfWE"},"source":["\n","#Complete Pipeline\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AuNrQsM_OaBA"},"outputs":[],"source":["def prepare_sales_forecasting_data(\n","    onpromotion=True,\n","    join_oil=True,\n","    join_transactions=True,\n","    start=\"20150701\",\n","    q=0.99,\n","    max_lags=28,\n","    dcoilwtico_forecast_df=None,\n","    onpromotion_forecast_df=None,\n","    transactions_forecast_df=None,\n","    n_jobs=-1,\n","    verbose=True):\n","\n","    # load the training, test, and transactions data sets\n","    _train_df = load_train_df(\"train.csv\")\n","\n","    # preprocess the data\n","    data_preprocessing_pipeline = make_data_preprocessing_pipeline(\n","        onpromotion,\n","        join_oil,\n","        join_transactions,\n","        start,\n","        q,\n","        verbose\n","    )\n","    _preprocessed_df = data_preprocessing_pipeline.fit_transform(_train_df)\n","\n","    # forecast future values of exogenous variables\n","    if dcoilwtico_forecast_df is None:\n","        dcoilwtico_forecast_df = forecast_dcoilwtico()\n","    if onpromotion_forecast_df is None:\n","        onpromotion_forecast_df = forecast_onpromotion()\n","    if transactions_forecast_df is None:\n","        transactions_forecast_df = forecast_transactions()\n","\n","    # engineer features\n","    feature_engineering_feature_union = make_feature_engineering_feature_union(\n","        max_lags,\n","        dcoilwtico_forecast_df,\n","        onpromotion_forecast_df,\n","        transactions_forecast_df,\n","        n_jobs,\n","        verbose\n","    )\n","    _with_engineered_features_df = feature_engineering_feature_union.fit_transform(\n","        _preprocessed_df\n","    )\n","\n","    # prepate for nixlats\n","    nixtlats_preparation_pipeline = make_nixtlats_preparation_pipeline(\n","        verbose\n","    )\n","    train_df, test_df = nixtlats_preparation_pipeline.fit_transform(_with_engineered_features_df)\n","\n","    return train_df, test_df"]},{"cell_type":"markdown","metadata":{"id":"Z83uoV9nOQkd"},"source":["#Helper Function for Extracting Static Feature Names\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rr306tU1OSfX"},"outputs":[],"source":["def get_static_feature_names(df) -\u003e list:\n","    is_static_feature = (\n","        df.columns.str.startswith(\"city_\") +\n","        df.columns.str.startswith(\"state_\") +\n","        df.columns.str.startswith(\"store_nbr_\") +\n","        df.columns.str.startswith(\"store_type_\") +\n","        df.columns.str.startswith(\"store_cluster\") +\n","        df.columns.str.startswith(\"family_\")\n","    )\n","    static_feature_names = (\n","        df.columns[is_static_feature]\n","          .to_list()\n","    )\n","    return static_feature_names\n"]},{"cell_type":"markdown","metadata":{"id":"n97QO0DqOFsb"},"source":["#Example Usage\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9YEfgja_OGgT"},"outputs":[],"source":["train_df, test_df = prepare_sales_forecasting_data()\n","train_df.info()\n","test_df.info()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMaNwSjP7EjpOg7Pkf7CP+Y","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}